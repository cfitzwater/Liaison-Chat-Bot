# Project Design Document: Liaison Library Bot

## Section 1: Project Overview
**Project Title:** Liaison Library Bot (RAG-Enabled Clinical Assistant)  
**Application Purpose:** The Liaison Library Bot is a Retrieval-Augmented Generation (RAG) application designed to serve as an intelligent interface for the "The Real Liaison Team Chat" document repository. In healthcare liaison work, critical information is often buried in disparate PDFs, insurance grids, and SOPs. This application indexes those unstructured files into a vector database, allowing users to ask natural language questions and receive conversational answers grounded specifically in the department's approved documentation. This ensures "source-of-truth" accuracy while eliminating manual search time.

**Intended User:** Clinical Liaisons and Registered Nurse Liaisons who need immediate, cited access to facility protocols and insurance eligibility guidelines during the patient intake process.

## Section 2: Core Features
* **Document Ingestion & Chunking:** A Python pipeline that uses `data_ingestion.py` to parse PDFs and break them into contextual segments.
* **Vector Database (ChromaDB):** Uses a persistent local database to store text along with high-dimensional embeddings generated by the `all-MiniLM-L6-v2` transformer model.
* **AI-Powered Semantic Search:** Finds relevant documents based on the "meaning" of the user's query rather than simple keyword matching.
* **LLM Integration (Gemini 2.5 Flash):** Leverages Google's Gemini API to synthesize retrieved documents into professional, cited answers.

## Section 3: Data Model
The structure of a single **Document Chunk** record within the ChromaDB collection is defined as follows:

| Field Name | Data Type | Description |
| :--- | :--- | :--- |
| **ids** | UUID String | A unique identifier for the specific segment of text. |
| **documents** | Text | The actual string of text extracted from the document. |
| **embeddings** | List (Float) | The 384-dimensional numerical representation of the text. |
| **metadatas** | Dictionary | Stores `source` (filename) and `page` (page number) for citations. |

---

## Chunk 1: CLI Application Prototype
This section documents the functional prototype developed as a Command Line Interface (CLI).

### Prototype Requirements
* **Text-Based Menu:** A clear interface allowing users to navigate between adding documents, viewing data, searching, and exiting.
* **Continuous Loop:** The program remains active until the user chooses to exit.
* **Modular Design:** Dedicated functions for `add_document()`, `display_all_chunks()`, and `search_documents()`.
* **Persistent Data Management:** Utilizes **ChromaDB** for persistent storage across sessions.

## Chunk 2: CLI Application Refactor (Data Integrity & Lifecycle)
This section documents the transition to a production-grade data management system.

### Refactor Objectives
* **Structured Data Mapping:** Transitioned to a formal List of Dictionaries pattern for cohesive metadata management.
* **Persistent State Synchronization:** Implemented Startup Sync logic to "rehydrate" the Python list from ChromaDB.
* **CRUD Lifecycle Implementation:** Expanded interface to include Export (Save) and Delete (Clear) capabilities.
* **Interoperability:** Added JSON Export feature for clinical auditing.

## Chunk 3: Data Persistence & Technical Challenge Compliance
This section documents the technical refinement for external file persistence.

### Persistence Requirements Covered
* **Startup File Verification:** Automatically checks for `library_data.json` upon launch.
* **Graceful Degradation:** Initializes empty datasets safely if files are missing.
* **Immediate Write-Back:** Every modification triggers an immediate update to the external JSON file.
* **Structured Human-Readable Storage:** Uses `json.dump` with indentation for auditable storage.

## Chunk 4: Flask Web Application & Professional UX Polish
This section documents the transition to a full-stack **Flask Web Application**, incorporating professional healthcare branding and advanced API error handling.

### Web Refactor Objectives
* **Branded Web Interface:** Modernized the UI using **Intermountain Healthcare** brand colors (Navy #0C0050 and Blue #3388DD).
* **API Resilience (Rate Limit Handling):** Implemented an **Exponential Backoff** retry loop to manage Gemini Free Tier limits, preventing crashes during high-traffic periods.
* **Deep-Link Citations:** Developed a RAG pipeline that generates clickable **Markdown Hyperlinks**. These use the `#page=X` suffix to open PDFs directly to the cited page.
* **Asynchronous UX:** Integrated a "Thinking" indicator and **Auto-Scroll** logic for real-time feedback during AI synthesis.

### Technical Implementation Details
| Feature | Implementation Detail |
| :--- | :--- |
| **Framework** | **Flask** (Backend) and **Marked.js** (Frontend Markdown rendering). |
| **Model** | **Gemini 2.5 Flash**â€”optimized for speed and reliability. |
| **Search Depth** | Configured `n_results=4` to capture cross-references across multiple document pages. |
| **Security** | Used `.env` for key management and `target="_blank"` for secure external link handling. |

## Chunk 5: Knowledge Entry System & Form Data Handling
This section documents the implementation of the manual knowledge entry portal, meeting the technical requirements for form handling and data extraction.

### Technical Challenge Implementation
* **Dynamic HTML Entry Form:** Developed a dedicated `/add` route that serves an HTML form for capturing user names, email addresses, and clinical notes.
* **POST Request Processing:** Implemented a `/save_item` route to handle incoming form data via **POST** requests.
* **Data Structuring:** The application extracts raw form data and structures it into a **Python Dictionary** before saving, ensuring data integrity.
* **Dual-Layer Storage:** Submitted items are saved to the persistent `library_data.json` file and immediately vectorized/indexed into **ChromaDB** to make the information searchable in real-time.
* **Advanced Session Management:** Integrated **JavaScript** and multi-tab redirection to ensure the user's active chatbot session remains intact during the entry process.