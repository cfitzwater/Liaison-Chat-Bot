# Project Design Document: Liaison Library Bot

## Section 1: Project Overview
**Project Title:** Liaison Library Bot (RAG-Enabled Clinical Assistant)  
**Application Purpose:** The Liaison Library Bot is a Retrieval-Augmented Generation (RAG) application designed to serve as an intelligent interface for the "The Real Liaison Team Chat" document repository. In healthcare liaison work, critical information is often buried in disparate PDFs, insurance grids, and SOPs. This application indexes those unstructured files into a vector database, allowing users to ask natural language questions and receive conversational answers grounded specifically in the department's approved documentation. This ensures "source-of-truth" accuracy while eliminating manual search time.

**Intended User:** Clinical Liaisons and Registered Nurse Liaisons who need immediate, cited access to facility protocols and insurance eligibility guidelines during the patient intake process.

## Section 2: Core Features
* **Document Ingestion & Chunking:** A Python pipeline that uses `data_ingestion.py` to parse PDFs and break them into contextual segments.
* **Vector Database (ChromaDB):** Uses a persistent local database to store text along with high-dimensional embeddings generated by the `all-MiniLM-L6-v2` transformer model.
* **AI-Powered Semantic Search:** Finds relevant documents based on the "meaning" of the user's query rather than simple keyword matching.
* **LLM Integration (Gemini 2.5 Flash):** Leverages Google's Gemini API to synthesize retrieved documents into professional, cited answers.

## Section 3: Data Model
The structure of a single **Document Chunk** record within the ChromaDB collection is defined as follows:

| Field Name | Data Type | Description |
| :--- | :--- | :--- |
| **ids** | UUID String | A unique identifier for the specific segment of text. |
| **documents** | Text | The actual string of text extracted from the document. |
| **embeddings** | List (Float) | The 384-dimensional numerical representation of the text. |
| **metadatas** | Dictionary | Stores `source` (filename) and `page` (page number) for citations. |

---

## Chunk 1: CLI Application Prototype
This section documents the functional prototype developed as a Command Line Interface (CLI).

### Prototype Requirements
* **Text-Based Menu:** A clear interface allowing users to navigate between adding documents, viewing data, searching, and exiting.
* **Continuous Loop:** The program remains active until the user chooses to exit.
* **Modular Design:** Dedicated functions for `add_document()`, `display_all_chunks()`, and `search_documents()`.
* **Persistent Data Management:** Utilizes **ChromaDB** for persistent storage across sessions.

## Chunk 2: CLI Application Refactor (Data Integrity & Lifecycle)
This section documents the transition to a production-grade data management system.

### Refactor Objectives
* **Structured Data Mapping:** Transitioned to a formal List of Dictionaries pattern for cohesive metadata management.
* **Persistent State Synchronization:** Implemented Startup Sync logic to "rehydrate" the Python list from ChromaDB.
* **CRUD Lifecycle Implementation:** Expanded interface to include Export (Save) and Delete (Clear) capabilities.
* **Interoperability:** Added JSON Export feature for clinical auditing.

## Chunk 3: Data Persistence & Technical Challenge Compliance
This section documents the technical refinement for external file persistence.

### Persistence Requirements Covered
* **Startup File Verification:** Automatically checks for `library_data.json` upon launch.
* **Graceful Degradation:** Initializes empty datasets safely if files are missing.
* **Immediate Write-Back:** Every modification triggers an immediate update to the external JSON file.
* **Structured Human-Readable Storage:** Uses `json.dump` with indentation for auditable storage.

## Chunk 4: Flask Web Application & Professional UX Polish
This section documents the transition to a full-stack **Flask Web Application**, incorporating professional healthcare branding and advanced API error handling.

### Web Refactor Objectives
* **Branded Web Interface:** Modernized the UI using **Intermountain Healthcare** brand colors (Navy #0C0050 and Blue #3388DD).
* **API Resilience (Rate Limit Handling):** Implemented an **Exponential Backoff** retry loop to manage Gemini Free Tier limits, preventing crashes during high-traffic periods.
* **Deep-Link Citations:** Developed a RAG pipeline that generates clickable **Markdown Hyperlinks**. These use the `#page=X` suffix to open PDFs directly to the cited page.
* **Asynchronous UX:** Integrated a "Thinking" indicator and **Auto-Scroll** logic for real-time feedback during AI synthesis.

### Technical Implementation Details
| Feature | Implementation Detail |
| :--- | :--- |
| **Framework** | **Flask** (Backend) and **Marked.js** (Frontend Markdown rendering). |
| **Model** | **Gemini 2.5 Flash**â€”optimized for speed and reliability. |
| **Search Depth** | Configured `n_results=4` to capture cross-references across multiple document pages. |
| **Security** | Used `.env` for key management and `target="_blank"` for secure external link handling. |

## Section 1: Project Overview
**Project Title:** Liaison Library Bot (RAG-Enabled Clinical Assistant)  
**Application Purpose:** The Liaison Library Bot is a Retrieval-Augmented Generation (RAG) application designed to serve as an intelligent interface for the "The Real Liaison Team Chat" document repository. In healthcare liaison work, critical information is often buried in disparate PDFs, insurance grids, and SOPs. This application indexes those unstructured files into a vector database, allowing users to ask natural language questions and receive conversational answers grounded specifically in the department's approved documentation. This ensures "source-of-truth" accuracy while eliminating manual search time.

**Intended User:** Clinical Liaisons and Registered Nurse Liaisons who need immediate, cited access to facility protocols and insurance eligibility guidelines during the patient intake process.

## Section 2: Core Features
* **Document Ingestion & Chunking:** A Python pipeline that uses `data_ingestion.py` to parse PDFs and break them into contextual segments.
* **Vector Database (ChromaDB):** Uses a persistent local database to store text along with high-dimensional embeddings generated by the `all-MiniLM-L6-v2` transformer model.
* **AI-Powered Semantic Search:** Instead of keyword matching, the bot finds relevant documents based on the "meaning" of the user's query.
* **LLM Integration (Gemini Pro):** Leverages Google's Gemini API to synthesize retrieved documents into a professional, cited answer.



## Section 3: Data Model
The structure of a single **Document Chunk** record within the ChromaDB collection is defined as follows:

| Field Name | Data Type | Description |
| :--- | :--- | :--- |
| **ids** | UUID String | A unique identifier for the specific segment of text. |
| **documents** | Text | The actual string of text extracted from the document. |
| **embeddings** | List (Float) | The 384-dimensional numerical representation of the text. |
| **metadatas** | Dictionary | Stores `source` (filename) and `page` (page number) for citations. |

---

## Chunk 1: CLI Application Prototype
This section documents the functional prototype of the Liaison Library Bot, developed as a Command Line Interface (CLI) as per technical requirements.

### Prototype Requirements
* **Text-Based Menu:** A clear interface allowing users to navigate between adding documents, viewing data, searching, and exiting.
* **Continuous Loop:** The program remains active, re-displaying the menu after each action until the user chooses option 4 (Exit).
* **Modular Design:** Dedicated functions for `add_document()`, `display_all_chunks()`, and `search_documents()`.
* **Persistent Data Management:** While the script runs a loop, it utilizes **ChromaDB** for persistent storage, ensuring data remains available even after the program restarts.

### Technical Implementation
The current `app.py` script serves as the primary controller. It orchestrates the following flow:
1. **Embedding Generation:** Uses `SentenceTransformer` to convert raw text into a format the computer can "understand."
2. **Contextual Retrieval:** When searching, the top 5 most relevant chunks are retrieved from the vector store.
3. **Augmented Generation:** The retrieved chunks are passed into a strict prompt for **Gemini-Pro**, forcing the AI to only use the provided healthcare documentation to answer, ensuring clinical accuracy.

| Function | Requirement Covered |
| :--- | :--- |
| `main()` | Continuous Loop / Menu Logic |
| `add_document()` | Add new item functionality |
| `display_all_chunks()` | List all items functionality |
| `search_documents()` | Advanced RAG search & AI Generation |

## Chunk 2: CLI Application Refactor (Data Integrity & Lifecycle)
This section documents the transition from a simple prototype to a production-grade data management system, focusing on data integrity, state synchronization, and the implementation of structured data models.

### Refactor Objectives
* **Structured Data Mapping:** Transitioned from managing loose text strings to a formal List of Dictionaries pattern. This ensures that metadata (source/page) and content are treated as a single, cohesive object.
* **Persistent State Synchronization:** Implemented a Startup Sync logic. The application now "rehydrates" the internal Python list from the persistent ChromaDB store upon launch, ensuring parity between the database and the user interface.
* **CRUD Lifecycle Implementation:** Expanded the interface to include a full data lifecycle, specifically adding Export (Save) and Delete (Clear) capabilities.
* **Interoperability:** Added a JSON Export feature, allowing clinical data to be moved between systems or audited by clinical reviewers in a human-readable format.

## Chunk 3: Data Persistence & Technical Challenge Compliance
This section documents the final technical refinement required to solve the problem of data loss using external file persistence. This update ensures the application meets the formal requirements for automated data loading and immediate storage.

### Persistence Requirements Covered
* **Startup File Verification:** The application automatically checks for the existence of `library_data.json` upon launch using `os.path.exists()`.
* **Graceful Degradation:** If the data file is missing, the application initializes an empty dataset without crashing.
* **Immediate Write-Back:** Every modification to the library (adding or clearing data) triggers an immediate update to the external JSON file.
* **Structured Human-Readable Storage:** Data is stored using JSON with an indentation level of 4, ensuring the file is easily auditable by clinical staff or instructors.

### Technical Implementation Details
| Feature | Requirement Met | Implementation Detail |
| :--- | :--- | :--- |
| **Auto-Load** | Load on start | `upload_data_from_file()` runs during initialization to hydrate the Master List. |
| **Immediate Save** | Modify/Write Immediately | `save_data_to_file()` is called directly inside `add_document()` and `clear_library()`. |
| **JSON Consistency** | Human-Readable Format | Uses `json.dump(..., indent=4)` to maintain a structured external file. |
| **Error Handling** | No-Crash Startup | `try-except` blocks wrap file I/O operations to handle corrupted or missing files safely. |
This section documents the transition from a simple prototype to a production-grade data management system, focusing on data integrity, state synchronization, and the implementation of structured data models.

### Refactor Objectives
* **Structured Data Mapping:** Transitioned from managing loose text strings to a formal List of Dictionaries pattern. This ensures that metadata (source/page) and content are treated as a single, cohesive object.
* **Persistent State Synchronization:** Implemented a Startup Sync logic. The application now "rehydrates" the internal Python list from the persistent ChromaDB store upon launch, ensuring parity between the database and the user interface.
* **CRUD Lifecycle Implementation:** Expanded the interface to include a full data lifecycle, specifically adding Export (Save) and Delete (Clear) capabilities.
* **Interoperability:** Added a JSON Export feature, allowing clinical data to be moved between systems or audited by clinical reviewers in a human-readable format.
```